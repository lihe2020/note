Spark是一种基于内存的快速、通用、可扩展的大数据分析引擎。

### 回顾历史

Hadoop诞生于Google的2篇论文，主要包括HDFS和MapReduce。其中MR有3大难以忍受的缺点：

1. MR基于数据集的计算，基于运算规则从存储介质中获取（采集）数据，然后进行计算，最后将结果存储到介质中，所以主要应用于**一次性计算**，不适合于数据挖掘和机器学习这样的迭代计算和图形挖掘计算。
2. MR基于文件存储介质的操作，性能非常的慢
3. MR和Hadoop紧密耦合在一起，无法动态替换（2013年10月份发布的2.x版本，推出了资源调度框架Yarn，解决了该问题）。

Spark发布于2013年6月份，它基于内存，并且使用Scala开发，天生适合迭代式计算



### Spark架构

1. Driver
2. Executor 



##### 运行模式

1. Local 本地模式
   1. loccal 所有计算都运行在一个线程当中，没有任何并行计算，通常用于测试，练手
   2. local[K] 指定使用几个线程来运行计算
   3. local[\*] 按cpu核书来设置线程，\*代表cpu核数
2. 部署到YARN
3. Stand alone模式



### RDD

RDD(Resilient Distributed Dataset)叫弹性分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、不分区、里面的元素可并行计算的集合。



算子：从认知心理学角度，解决问题其实将问题的初始状态，通过一系列的操作（Operate）（算子）对问题的状态进行转换，然后达到完成（解决）状态。



Spark中的所有的RDD方法都成为算子，但分为2大类：

- **转换算子**，转换结构
- **行动算子**，做实际的操作



创建RDD有3种方式：

1. 从集合中创建RDD，主要有2种函数：`parallelize`、`makeRDD`
2. 从外部存储中创建RDD
3. 从其他RDD中创建







