这篇文章主要描述了知乎已读服务（类似于过滤曝光历史）的架构演进过程，从分析的PPT来看他们存储了最近3年的已读历史，基于庞大的用户群体来推测，他们存储的数据是海量的：

- 使用redis+bloom filter，这种方式缺乏对多个位的批量操作，并且全部使用这种方式对内存的占用非常大，成本高昂。同时这种方式无法合理控制Bloom Filter的尺寸和False Positive Rate。
- 使用hbase，这种方式成本低廉，使用于大数据场景，但是用户曝光的数据数据稀疏数据，会涉及到大量随机IO（我自己脑补的，待验证），影响到HBASE的缓存命中率，而此时对存储的访问需要的IO路径很长，而这种情况下可能会对延迟相关指标造成较大幅度的影响，对于这种不稳定的情况是难以接受的。
- 多级缓存+tikv

这个可以作为面试的时候吹牛的点。但是**前提是需要了解布隆过滤的所有细节**，目前我还不清楚布隆过滤算法到底是怎么实现的。

> 关于知乎的文章中，还有另一篇值得学习的就是他们用go重构了在线召回服务，然后写了一篇文章介绍了整个流程，地址：https://zhuanlan.zhihu.com/p/53130925。

参考：

- 知乎文章：https://zhuanlan.zhihu.com/p/68383301
- 新东方直播：http://roombox.xdf.cn/web/1602438315
- 白话布隆过滤器：https://mp.weixin.qq.com/s/B2L_owH5vIaXuXvnGxq0kQ