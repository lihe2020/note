随着深度模型越来越复杂，训练时间和成本也是不容忽视的问题，为了让模型尽快收敛，分布式训练成为一个日趋热门的话题。



## 同步策略

为了能更好的了解分布式训练，需要了解一些多GPU通信概念。主流的多GPU通信框架(如[NCCL](https://github.com/NVIDIA/nccl))采用了[MPI](https://mpitutorial.com/tutorials/)协议通信。MPI中有如下重要的概念：

***Allreduce***

![mpi_allreduce_1](../images/mpi_allreduce_1.png)

## TensorFlow

1. `MirroredStrategy` 这是一种简单高效的分布式策略，主要用于单机多卡，主要步骤：
   1. 开始训练前，将模型复制到各GPU
   2. 每次训练时，将一个batch的数据分成N份，分别传输到各GPU（数据并行），各GPU各自计算这部分数据的梯度
   3. 计算完成后，各GPU使用Allreduce同步策略相互交换梯度
   4. 各GPU将梯度进行平均(求和)，并更新本地变量
2. `CentralStorageStrategy` 模型参数存储在CPU里，每次训练前同步到各GPU，训练完后梯度汇总到CPU，更新模型参数
3. `MultiWorkerMirroredStrategy` 是`MirroredStrategy`的多机多卡版本。不过涉及到多计算机的通信，所以还需要额外的环境配置`TF_CONFIG`。
4. `ParameterServerStrategy`



## 参考

1. [TensorFlow 分布式训练 ](https://tf.wiki/zh/appendix/distributed.html)
2. [单机多卡的正确打开方式](https://fyubang.com/2019/07/08/distributed-training/)
3. [使用 tf.distribute 扩展 TensorFlow](https://www.bilibili.com/video/av68131040)